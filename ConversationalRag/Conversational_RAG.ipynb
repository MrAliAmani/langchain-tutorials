{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Conversational RAG"
      ],
      "metadata": {
        "id": "eXcRDTllT6Q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-chroma bs4 langchain-groq langchain-google-genai langgraph"
      ],
      "metadata": {
        "id": "chybAFHtUEWy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.runnables.config import RunnableConfig\n",
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSVjw-1KVmwY",
        "outputId": "acb66a5c-e46e-4d67-e755-ba0a00580420"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LANGCHAIN_API_KEY = userdata.get('LANGCHAIN_API_KEY')\n",
        "LANGCHAIN_TRACING_V2 = userdata.get('LANGCHAIN_TRACING_V2')\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "onSymgMBV5GT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(model=\"llama3-8b-8192\", groq_api_key=GROQ_API_KEY, temperature=0)"
      ],
      "metadata": {
        "id": "AbRouJu1Vw4s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load, chunk and index the contents of the blog to create a retriever.\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=GoogleGenerativeAIEmbeddings(\n",
        "        model=\"models/embedding-001\",\n",
        "        google_api_key=GOOGLE_API_KEY\n",
        "        )\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "\n",
        "# 2. Incorporate the retriever into a question-answering chain.\n",
        "system_prompt = (\n",
        "    \"You are an assistant for question-answering tasks. \"\n",
        "    \"Use the following pieces of retrieved context to answer \"\n",
        "    \"the question. If you don't know the answer, say that you \"\n",
        "    \"don't know. Use three sentences maximum and keep the \"\n",
        "    \"answer concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
      ],
      "metadata": {
        "id": "WFHvSjnuWcJY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke({\"input\": \"What is Task Decomposition?\"})\n",
        "response[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "f0dhC29vWcZS",
        "outputId": "487750db-c9c2-428d-cc40-f33a06c10062"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Task Decomposition is a process that breaks down a complicated task into smaller and simpler steps. This is achieved by using techniques such as Chain of Thought (CoT) or Tree of Thoughts (Yao et al. 2023), which help to decompose hard tasks into manageable tasks and provide an interpretation of the model's thinking process.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding chat history"
      ],
      "metadata": {
        "id": "jYC1cn0QXEqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "contextualize_q_system_prompt = (\n",
        "    \"Given a chat history and the latest user question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a standalone question which can be understood \"\n",
        "    \"without the chat history. Do NOT answer the question, \"\n",
        "    \"just reformulate it if needed and otherwise return it as is.\"\n",
        ")\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, contextualize_q_prompt\n",
        ")"
      ],
      "metadata": {
        "id": "jNHM6cvPXwVG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
      ],
      "metadata": {
        "id": "-WoKWY7GYc6j"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "question = \"What is Task Decomposition?\"\n",
        "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
        "chat_history.extend(\n",
        "    [\n",
        "        HumanMessage(content=question),\n",
        "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "second_question = \"What are common ways of doing it?\"\n",
        "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
        "\n",
        "print(ai_msg_2[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtZG0LwMYioV",
        "outputId": "0cf0fb6c-2cf0-469f-91ed-4cf9b3dd244b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common ways of doing Task Decomposition include using Large Language Models (LLM) with simple prompting, task-specific instructions, or human inputs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stateful management of chat history"
      ],
      "metadata": {
        "id": "5BO2c0xwY7hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "store = {}\n",
        "\n",
        "\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "\n",
        "conversational_rag_chain = RunnableWithMessageHistory(\n",
        "    rag_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        "    output_messages_key=\"answer\",\n",
        ")"
      ],
      "metadata": {
        "id": "A9EnAKv3ZHD1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversational_rag_chain.invoke(\n",
        "    {\"input\": \"What is Task Decomposition?\"},\n",
        "    config={\n",
        "        \"configurable\": {\"session_id\": \"abc123\"}\n",
        "    },  # constructs a key \"abc123\" in `store`.\n",
        ")[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "qB6B6zq7ZX3P",
        "outputId": "1ccde57f-7564-4f46-c98b-9ed7136138df"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Task Decomposition is a process that breaks down a complicated task into smaller and simpler steps. This is achieved by using techniques such as Chain of Thought (CoT) or Tree of Thoughts (Yao et al. 2023), which help to decompose hard tasks into manageable tasks and provide an interpretation of the model's thinking process.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversational_rag_chain.invoke(\n",
        "    {\"input\": \"What are common ways of doing it?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
        ")[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "l_lxpjkSZbHB",
        "outputId": "7b9a9174-cd3e-42f1-a7cb-3c595eff43c0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Common ways of doing Task Decomposition include using Large Language Models (LLM) with simple prompting, task-specific instructions, or human inputs.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for message in store[\"abc123\"].messages:\n",
        "    if isinstance(message, AIMessage):\n",
        "        prefix = \"AI\"\n",
        "    else:\n",
        "        prefix = \"User\"\n",
        "\n",
        "    print(f\"{prefix}: {message.content}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVCqQRBwZgxr",
        "outputId": "13d684a7-499b-4fff-86ac-37309bfe7b7c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: What is Task Decomposition?\n",
            "\n",
            "AI: Task Decomposition is a process that breaks down a complicated task into smaller and simpler steps. This is achieved by using techniques such as Chain of Thought (CoT) or Tree of Thoughts (Yao et al. 2023), which help to decompose hard tasks into manageable tasks and provide an interpretation of the model's thinking process.\n",
            "\n",
            "User: What are common ways of doing it?\n",
            "\n",
            "AI: Common ways of doing Task Decomposition include using Large Language Models (LLM) with simple prompting, task-specific instructions, or human inputs.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval tool"
      ],
      "metadata": {
        "id": "VbJhVglvZrvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools.retriever import create_retriever_tool\n",
        "\n",
        "tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"blog_post_retriever\",\n",
        "    \"Searches and returns excerpts from the Autonomous Agents blog post.\",\n",
        ")\n",
        "tools = [tool]"
      ],
      "metadata": {
        "id": "IJHexZWfZ8Zx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool.invoke(\"task decomposition\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "cRKIoW7zaCU6",
        "outputId": "80624466-6142-46d0-88ad-cada5635a584"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent constructor"
      ],
      "metadata": {
        "id": "Zi56S6cmaI-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "agent_executor = create_react_agent(llm, tools)"
      ],
      "metadata": {
        "id": "o564OAJkaM8h"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Explain the concept of Task Decomposition.\"\n",
        "\n",
        "for s in agent_executor.stream(\n",
        "    {\"messages\": [HumanMessage(content=query)]},\n",
        "):\n",
        "    print(s)\n",
        "    print(\"----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VgDaCQMaPg3",
        "outputId": "2996b0bd-42b6-49ab-9a23-2f6f0418612a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_29cd', 'function': {'arguments': '{\"query\":\"Task Decomposition\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 428, 'prompt_tokens': 929, 'total_tokens': 1357, 'completion_time': 0.356666667, 'prompt_time': 0.189511988, 'queue_time': None, 'total_time': 0.546178655}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-8d441aa4-064c-4a77-b92a-5b75e12a1ee4-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'Task Decomposition'}, 'id': 'call_29cd', 'type': 'tool_call'}], usage_metadata={'input_tokens': 929, 'output_tokens': 428, 'total_tokens': 1357})]}}\n",
            "----\n",
            "{'tools': {'messages': [ToolMessage(content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.', name='blog_post_retriever', tool_call_id='call_29cd')]}}\n",
            "----\n",
            "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_f1nd', 'function': {'arguments': '{\"query\":\"Task Decomposition\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 214, 'prompt_tokens': 1486, 'total_tokens': 1700, 'completion_time': 0.178333333, 'prompt_time': 0.301123436, 'queue_time': None, 'total_time': 0.479456769}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-4651f81a-0def-4756-b011-839240fb348b-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'Task Decomposition'}, 'id': 'call_f1nd', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1486, 'output_tokens': 214, 'total_tokens': 1700})]}}\n",
            "----\n",
            "{'tools': {'messages': [ToolMessage(content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.', name='blog_post_retriever', tool_call_id='call_f1nd')]}}\n",
            "----\n",
            "{'agent': {'messages': [AIMessage(content='Task decomposition is a process of breaking down a complex task into smaller, more manageable subtasks. This is often done to make the task more understandable, to reduce the complexity of the task, and to make it easier to plan and execute.\\n\\nThere are several techniques that can be used for task decomposition, including:\\n\\n1. Chain of Thought (CoT): This involves instructing the model to \"think step by step\" to decompose hard tasks into smaller and simpler steps.\\n2. Tree of Thoughts: This extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure.\\n3. Task-specific instructions: This involves using task-specific instructions, such as \"Write a story outline.\" for writing a novel.\\n4. Human inputs: This involves using human inputs to decompose the task.\\n\\nThe process of task decomposition typically involves four stages:\\n\\n1. Task planning: This involves parsing the user request into multiple tasks and identifying the attributes associated with each task, such as task type, ID, dependencies, and arguments.\\n2. Model selection: This involves selecting the appropriate model or expert to execute the task.\\n3. Task execution: This involves executing the task using the selected model or expert and logging the results.\\n4. Task analysis: This involves describing the process and results of the task, including any analysis and model inference results.\\n\\nBy breaking down a complex task into smaller subtasks, task decomposition can make it easier to plan and execute the task, and can provide a clearer understanding of the task and its components.', response_metadata={'token_usage': {'completion_tokens': 307, 'prompt_tokens': 2043, 'total_tokens': 2350, 'completion_time': 0.255833333, 'prompt_time': 0.306895753, 'queue_time': None, 'total_time': 0.562729086}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_873a560973', 'finish_reason': 'stop', 'logprobs': None}, id='run-3b299b2f-3a26-4c18-ac71-56defa2bed52-0', usage_metadata={'input_tokens': 2043, 'output_tokens': 307, 'total_tokens': 2350})]}}\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "memory = MemorySaver()\n",
        "\n",
        "agent_executor = create_react_agent(llm, tools, checkpointer=memory)"
      ],
      "metadata": {
        "id": "owmmGgwvaY-h"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = RunnableConfig(configurable={\"thread_id\": \"abc123\"})\n",
        "\n",
        "for s in agent_executor.stream(\n",
        "    {\"messages\": [HumanMessage(content=\"Hi! I'm bob\")]}, config=config\n",
        "):\n",
        "    print(s)\n",
        "    print(\"----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HVzfLM5ahrY",
        "outputId": "1b7a10e1-c7b3-45e8-fbf4-148f2bcfae41"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'agent': {'messages': [AIMessage(content='Hi Bob!', response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 946, 'total_tokens': 950, 'completion_time': 0.003333333, 'prompt_time': 0.142477789, 'queue_time': None, 'total_time': 0.145811122}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_873a560973', 'finish_reason': 'stop', 'logprobs': None}, id='run-3105e795-20db-417a-8a40-35f353308b5b-0', usage_metadata={'input_tokens': 946, 'output_tokens': 4, 'total_tokens': 950})]}}\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Could you please explain the concept of Task Decomposition?\"\n",
        "\n",
        "for s in agent_executor.stream(\n",
        "    {\"messages\": [HumanMessage(content=query)]}, config=config\n",
        "):\n",
        "    print(s)\n",
        "    print(\"----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfiHOPzIalaT",
        "outputId": "fe68034c-01f6-4914-d7ba-cd039217040f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_nvmq', 'function': {'arguments': '{\"query\":\"Task Decomposition\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 972, 'total_tokens': 1043, 'completion_time': 0.055789873, 'prompt_time': 0.193758423, 'queue_time': None, 'total_time': 0.249548296}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_af05557ca2', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d345ce34-a9d8-42b7-bfc2-65177d55289e-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'Task Decomposition'}, 'id': 'call_nvmq', 'type': 'tool_call'}], usage_metadata={'input_tokens': 972, 'output_tokens': 71, 'total_tokens': 1043})]}}\n",
            "----\n",
            "{'tools': {'messages': [ToolMessage(content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.', name='blog_post_retriever', tool_call_id='call_nvmq')]}}\n",
            "----\n",
            "{'agent': {'messages': [AIMessage(content='It seems like the tool call id \"call_nvmq\" yielded a detailed explanation of Task Decomposition, including its concept, techniques, and applications. It appears that the tool call was successful in providing a comprehensive overview of the topic.\\n\\nAs the conversation has already utilized a tool to provide information, I will respond directly to your question without using any additional tools.\\n\\nTask Decomposition is a problem-solving strategy that involves breaking down complex tasks into smaller, more manageable subtasks. This approach helps to simplify the task, reduce complexity, and make it easier to understand and execute. Task Decomposition can be achieved through various techniques, such as Chain of Thought, Tree of Thoughts, and human inputs.\\n\\nIn the context of artificial intelligence and machine learning, Task Decomposition is often used to enable machines to perform complex tasks by decomposing them into smaller tasks that can be executed by individual models or agents. This approach can improve the efficiency, accuracy, and scalability of complex tasks.\\n\\nI hope this direct response provides a clear understanding of Task Decomposition. If you have any further questions or would like to explore related topics, please feel free to ask!', response_metadata={'token_usage': {'completion_tokens': 229, 'prompt_tokens': 1506, 'total_tokens': 1735, 'completion_time': 0.190833333, 'prompt_time': 0.224749432, 'queue_time': None, 'total_time': 0.415582765}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-04a0ae2d-10b2-4798-ba59-86b6aa40477f-0', usage_metadata={'input_tokens': 1506, 'output_tokens': 229, 'total_tokens': 1735})]}}\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What according to the blog post are common ways of doing it? redo the search\"\n",
        "\n",
        "for s in agent_executor.stream(\n",
        "    {\"messages\": [HumanMessage(content=query)]}, config=config\n",
        "):\n",
        "    print(s)\n",
        "    print(\"----\")"
      ],
      "metadata": {
        "id": "yIwUaYsian4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12b0dfd6-89f2-4534-8fa9-9ffa2a9cdccd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_9f0m', 'function': {'arguments': '{\"query\":\"What are common ways of doing task decomposition?\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 1760, 'total_tokens': 1807, 'completion_time': 0.039166667, 'prompt_time': 0.327519013, 'queue_time': None, 'total_time': 0.36668568}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-55c13fd1-305b-4777-b3c9-50fdb3614f12-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'What are common ways of doing task decomposition?'}, 'id': 'call_9f0m', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1760, 'output_tokens': 47, 'total_tokens': 1807})]}}\n",
            "----\n",
            "{'tools': {'messages': [ToolMessage(content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\n\"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully', name='blog_post_retriever', tool_call_id='call_9f0m')]}}\n",
            "----\n",
            "{'agent': {'messages': [AIMessage(content='According to the blog post, common ways of doing task decomposition are:\\n\\n1. By LLM with simple prompting, such as \"Steps for XYZ.\\\\n1.\" or \"What are the subgoals for achieving XYZ?\"\\n2. By using task-specific instructions, for example, \"Write a story outline.\" for writing a novel\\n3. With human inputs', response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 2412, 'total_tokens': 2484, 'completion_time': 0.06, 'prompt_time': 0.370880785, 'queue_time': None, 'total_time': 0.430880785}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_873a560973', 'finish_reason': 'stop', 'logprobs': None}, id='run-96681a3a-80de-472f-84bf-dae6110db88f-0', usage_metadata={'input_tokens': 2412, 'output_tokens': 72, 'total_tokens': 2484})]}}\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JArMQpfuj1vA"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}