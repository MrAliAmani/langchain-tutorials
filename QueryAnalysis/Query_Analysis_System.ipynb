{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rIzvG1r3bgpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efd321a1-d793-4beb-e955-3477c8d0e5b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/103.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/103.5 kB\u001b[0m \u001b[31m962.4 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m102.4/103.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain langchain-community langchain_google_genai youtube-transcript-api pytube langchain-chroma langchain_groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "from typing import Optional, List\n",
        "import datetime\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_groq import ChatGroq"
      ],
      "metadata": {
        "id": "8i2O7ZREmj-2"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LANGCHAIN_API_KEY = userdata.get('LANGCHAIN_API_KEY')\n",
        "LANGCHAIN_TRACING_V2 = userdata.get('LANGCHAIN_TRACING_V2')\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "COHERE_API_KEY = userdata.get('COHERE_API_KEY')"
      ],
      "metadata": {
        "id": "HW5P2k3kmD-b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "    \"https://www.youtube.com/watch?v=HAn9vnJy6S4\",\n",
        "    \"https://www.youtube.com/watch?v=dA1cHGACXCo\",\n",
        "    \"https://www.youtube.com/watch?v=ZcEMLz27sL4\",\n",
        "    \"https://www.youtube.com/watch?v=hvAPnpSfSGo\",\n",
        "    \"https://www.youtube.com/watch?v=EhlPDL4QrWY\",\n",
        "    \"https://www.youtube.com/watch?v=mmBo8nlu2j0\",\n",
        "    \"https://www.youtube.com/watch?v=rQdibOsL1ps\",\n",
        "    \"https://www.youtube.com/watch?v=28lC4fqukoc\",\n",
        "    \"https://www.youtube.com/watch?v=es-9MgxB-uc\",\n",
        "    \"https://www.youtube.com/watch?v=wLRHwKuKvOE\",\n",
        "    \"https://www.youtube.com/watch?v=ObIltMaRJvY\",\n",
        "    \"https://www.youtube.com/watch?v=DjuXACWYkkU\",\n",
        "    \"https://www.youtube.com/watch?v=o7C9ld6Ln-M\",\n",
        "]\n",
        "docs = []\n",
        "for url in urls:\n",
        "    docs.extend(YoutubeLoader.from_youtube_url(url, add_video_info=True).load())"
      ],
      "metadata": {
        "id": "oWvU1iWJmYcE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add some additional metadata: what year the video was published\n",
        "for doc in docs:\n",
        "    doc.metadata[\"publish_year\"] = int(\n",
        "        datetime.datetime.strptime(\n",
        "            doc.metadata[\"publish_date\"], \"%Y-%m-%d %H:%M:%S\"\n",
        "        ).strftime(\"%Y\")\n",
        "    )"
      ],
      "metadata": {
        "id": "5cCF_COBmuJe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[doc.metadata[\"title\"] for doc in docs]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxFdw6nsm6XD",
        "outputId": "a717ffdd-7938-4512-c607-f4c8d213f456"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['OpenGPTs',\n",
              " 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve',\n",
              " 'Streaming Events: Introducing a new `stream_events` method',\n",
              " 'LangGraph: Multi-Agent Workflows',\n",
              " 'Build and Deploy a RAG app with Pinecone Serverless',\n",
              " 'Auto-Prompt Builder (with Hosted LangServe)',\n",
              " 'Build a Full Stack RAG App With TypeScript',\n",
              " 'Getting Started with Multi-Modal LLMs',\n",
              " 'SQL Research Assistant',\n",
              " 'Skeleton-of-Thought: Building a New Template from Scratch',\n",
              " 'Benchmarking RAG over LangChain Docs',\n",
              " 'Building a Research Assistant from Scratch',\n",
              " 'LangServe and LangChain Templates Webinar']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3Nv7O_Qm71N",
        "outputId": "54111a7a-6576-4b20-9e86-dc01a5b4cefd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': 'HAn9vnJy6S4',\n",
              " 'title': 'OpenGPTs',\n",
              " 'description': 'Unknown',\n",
              " 'view_count': 9189,\n",
              " 'thumbnail_url': 'https://i.ytimg.com/vi/HAn9vnJy6S4/hq720.jpg',\n",
              " 'publish_date': '2024-01-31 00:00:00',\n",
              " 'length': 1530,\n",
              " 'author': 'LangChain',\n",
              " 'publish_year': 2024}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
        "chunked_docs = text_splitter.split_documents(docs)\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
        "vectorstore = Chroma.from_documents(\n",
        "    chunked_docs,\n",
        "    embeddings,\n",
        ")"
      ],
      "metadata": {
        "id": "ZHtf1TYzm_jR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval without query analysis"
      ],
      "metadata": {
        "id": "kG2dWrFooVGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_results = vectorstore.similarity_search(\"how do I build a RAG agent\")\n",
        "print(search_results[0].metadata[\"title\"])\n",
        "print(search_results[0].page_content[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtycZbEVodWc",
        "outputId": "f1c443b7-c539-47cc-a371-bde31321938d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenGPTs\n",
            "hardcoded that it will always do a retrieval step here the assistant decides whether to do a retrieval step or not sometimes this is good sometimes this is bad sometimes it you don't need to do a retrieval step when I said hi it didn't need to call it tool um but other times you know the the llm might mess up and not realize that it needs to do a retrieval step and so the rag bot will always do a retrieval step so it's more focused there because this is also a simpler architecture so it's always\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_results = vectorstore.similarity_search(\"videos on RAG published in 2023\")\n",
        "print(search_results[0].metadata[\"title\"])\n",
        "print(search_results[0].metadata[\"publish_date\"])\n",
        "print(search_results[0].page_content[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRNjn9IbogPf",
        "outputId": "6279397a-6b2c-48eb-a6b0-28c24236a7c0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Benchmarking RAG over LangChain Docs\n",
            "2023-11-22 00:00:00\n",
            "hello all right today we're going to go over how to Benchmark some retrievable algorithms using the new uh Lang chain benchmarks package um as well as the public data set that we're releasing um specifically the public data set is for question answering over Lang chain documentation um and so we've provided uh about 86 examples of inputs and expected outputs um and as well as some utilities for doing some basic kind of like in question um and then we've evaluated a bunch of different methods for\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query analysis"
      ],
      "metadata": {
        "id": "IYuldsDVos8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query schema\n",
        "class Search(BaseModel):\n",
        "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
        "\n",
        "    query: str = Field(\n",
        "        ...,\n",
        "        description=\"Similarity search query applied to video transcripts.\",\n",
        "    )\n",
        "    publish_year: Optional[int] = Field(None, description=\"Year video was published\")"
      ],
      "metadata": {
        "id": "DRHfrcAEo-e6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query generation\n",
        "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
        "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
        "Given a question, return a list of database queries optimized to retrieve the most relevant results.\n",
        "\n",
        "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "llm = ChatGroq(model=\"mixtral-8x7b-32768\", temperature=0, api_key=GROQ_API_KEY)\n",
        "structured_llm = llm.with_structured_output(Search)\n",
        "query_analyzer = {\"question\": RunnablePassthrough()} | prompt | structured_llm"
      ],
      "metadata": {
        "id": "pvkZZajqpO2O"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_analyzer.invoke(\"how do I build a RAG agent\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y87y7-5qECH",
        "outputId": "90159fb5-2c0e-41b4-a0a3-7974594737a5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Search(query='build RAG agent', publish_year=None)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_analyzer.invoke(\"videos on RAG published in 2023\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3imKuvMZqJT7",
        "outputId": "4581d2ae-5c70-4e2e-aa4a-47bea8a54f82"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Search(query='videos on RAG', publish_year=2023)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval with query analysis"
      ],
      "metadata": {
        "id": "dV2ZvE1mqMpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieval(search: Search) -> List[Document]:\n",
        "    if search.publish_year is not None:\n",
        "        # This is syntax specific to Chroma,\n",
        "        # the vector database we are using.\n",
        "        _filter = {\"publish_year\": {\"$eq\": search.publish_year}}\n",
        "    else:\n",
        "        _filter = None\n",
        "    return vectorstore.similarity_search(search.query, filter=_filter)"
      ],
      "metadata": {
        "id": "tls5yR0sqPG3"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_chain = query_analyzer | retrieval"
      ],
      "metadata": {
        "id": "3gD0Iuv5qdxT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = retrieval_chain.invoke(\"RAG tutorial published in 2023\")"
      ],
      "metadata": {
        "id": "udOg4HUaqfSO"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[(doc.metadata[\"title\"], doc.metadata[\"publish_date\"]) for doc in results]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLFFogFTqhTT",
        "outputId": "47500edb-515a-4857-f48f-efca27167a4c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Getting Started with Multi-Modal LLMs', '2023-12-20 00:00:00'),\n",
              " ('LangServe and LangChain Templates Webinar', '2023-11-02 00:00:00'),\n",
              " ('Getting Started with Multi-Modal LLMs', '2023-12-20 00:00:00'),\n",
              " ('Benchmarking RAG over LangChain Docs', '2023-11-22 00:00:00')]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RF6ksiLUqn8X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}